{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/archivesunleashed/archivesunleashed.org/master/themes/hugo-material-docs/static/images/logo-square.png\" height=\"100px\" width=\"100px\">\n",
    "\n",
    "\n",
    "# Welcome\n",
    "\n",
    "Welcome to the Archives Unleashed Cloud Jupyter Notebook. This demonstration takes the main derivatives from the Cloud and uses Python to analyze and produce information about your collection.\n",
    "\n",
    "Please feel free to create an [issue](https://github.com/archivesunleashed/auk/issues) to let us know about any bugs you encountered or improvements you would like to see.\n",
    "\n",
    "If you have Python experience, please feel free to change the provided code to suit your own needs.\n",
    "\n",
    "We recommend that you use `File > Make a Copy` first before changing the code in the repository. That way, you can always return to the basic visualizations we have offered here. Of course, you can also just re-download the Jupyter Notebook file from your Archives Unleashed Cloud account.\n",
    "\n",
    "### How Jupyter Notebooks Work:\n",
    "\n",
    "If you have no previous experience with Jupyter Notebooks, the most important thing to understand is that that `<Shift> + <Enter/Return>` will run the Python code inside a cell and output it to below the cell.\n",
    "    \n",
    "The cells that cover the required inputs, marked \"Setup\", need to be run before the rest of the notebook will work. These cells will import all the libraries and set basic variables (e.g. where your derivative files are located) for the notebook. After that, everything else should be able to run on its own.\n",
    "\n",
    "If you just want to see all results for your collection, use `Cell > Run All`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Collection ID\n",
    "\n",
    "This variable is the most important, and the only variable you need to change to see a complete set of visualizations for your [Archives Unleashed Cloud derivatives](https://cloud.archivesunleashed.org/derivatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_ID = \"4867\" # Change to switch collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the collection id, the rest of the variables will be set up here. Also, some libraries like [pandas](https://pandas.pydata.org/), [numpy](http://www.numpy.org/), [networkx](https://networkx.github.io/), and [nltk](https://www.nltk.org/) will be imported so you can do the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages.\n",
    "\n",
    "from collections import Counter\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.draw.dispersion import dispersion_plot as dp\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Setup Archives Unleashed Cloud data.\n",
    "\n",
    "auk_fp = \"./data/\"\n",
    "auk_full_text = auk_fp + COLLECTION_ID + \"-fulltext.txt\"\n",
    "auk_gephi = auk_fp + COLLECTION_ID + \"-gephi.gexf\"\n",
    "auk_graphml = auk_fp + COLLECTION_ID + \"-gephi.graphml\"\n",
    "auk_domains = auk_fp + COLLECTION_ID + \"-fullurls.txt\"\n",
    "auk_filtered_text = auk_fp + COLLECTION_ID + \"-filtered_text.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Configuration\n",
    "\n",
    "The following cell sets out some user-generated variables. Take a look here: are there any domains you are not interested in? How many words would you like to be shown? Do you want to filter out 404 results? Do you want to sample the data? Read the choices below carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of words to show in output.\n",
    "# Jupyter will create an output error if the number is too high.\n",
    "\n",
    "TOP_COUNT = 30 \n",
    "\n",
    "# Domain suffixes to check non-U.S. domains so that (e.g.) www.google.co.uk \n",
    "# will become \"google\".\n",
    "\n",
    "STOP_DOMAINS = [\"co\", \"org\", \"net\", \"edu\"] # Domain suffixes to remove.\n",
    "\n",
    "# Minimum number of characters for a word to be included in a corpus.\n",
    "\n",
    "MINIMUM_WORD_LENGTH = 3 # Eliminates \"it\", \"I\", \"be\" etc.\n",
    "\n",
    "# List of substrings to filter a text line, if desired.\n",
    "\n",
    "LINE_FILTER = ['404 Not Found']\n",
    "\n",
    "# How many lines of text to use.\n",
    "\n",
    "RESULTS_LIMIT = 2500\n",
    "\n",
    "# If you want to start at a different line, you can increase this.\n",
    "# If RESULTS_START is great than RESULTS_LIMIT you will get no results.\n",
    "\n",
    "RESULTS_START = 0\n",
    "\n",
    "# If you have a large file but want to sample the file more broadly.\n",
    "# You can increase this value skip to every Nth line.\n",
    "\n",
    "RESULTS_STEP = 5\n",
    "\n",
    "# Change if you want a different filename.\n",
    "\n",
    "OUTPUT_FILENAME = \"./filtered_text.txt\" # filename if you want to output to another file.\n",
    "\n",
    "# Characters to show per text file in output.\n",
    "# Larger numbers will result in more text showing in output.\n",
    "\n",
    "MAX_CHARACTERS = 75\n",
    "\n",
    "# The years to include in the analysis.\n",
    "# If empty, you will get all available years.\n",
    "\n",
    "FILTERED_YEARS = [] # E.g. ['2015', '2016', '2019'].\n",
    "\n",
    "# The domains to include in the analysis.\n",
    "# If empty, you will get all available domains.\n",
    "\n",
    "FILTERED_DOMAINS = [] # E.g [\"google\", \"apple\", \"facebook\"].\n",
    "\n",
    "# List of words not to include in a corpus for text analysis.\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archives Unleashed Cloud Python Library\n",
    "\n",
    "The below cell now sets up the functions that drive the analysis throughout this notebook. If you don't run it, you won't be able to work with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domain(s):\n",
    "    \"\"\"Extracts the name from the domain (e.g. 'www.google.com' becomes 'google').\n",
    "    \n",
    "    :param: s: the domain name to clean.\n",
    "    :return: the relevant name.\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = \"\"\n",
    "    dom = s.split(\".\")\n",
    "    if len(dom) <3: # x.com is always x.\n",
    "        ret = dom[0]\n",
    "    elif dom[-2] in STOP_DOMAINS: # www.x.co.uk should be x.\n",
    "        ret = dom[-3]\n",
    "    else:\n",
    "        ret = dom[1]\n",
    "    return ret\n",
    "\n",
    "def get_domains(split_method=\"full\"):\n",
    "    \"\"\"Extracts the domains from a file by method.\n",
    "    \n",
    "    :param split_method: Either \"full\" \"name\" or \"sub\". \"name\" provides just the domain name, \n",
    "         \"sub\" produces the name with subdomains. \"full\" provides the entire name. \n",
    "    :return: a list of tuples containing (urlname, count).\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = []\n",
    "    with open(auk_domains) as fin:\n",
    "        for line in fin:\n",
    "            ret.append(line.strip('()\\n').split(\",\"))\n",
    "    if split_method == 'name':\n",
    "        scores = Counter()\n",
    "        for url, count in ret:\n",
    "            scores.update({clean_domain(url): int(count)})\n",
    "        ret = scores\n",
    "    elif split_method == 'sub':\n",
    "        scores = Counter()\n",
    "        splits = [(x[0].split('.'), int(x[1])) for x in ret]\n",
    "        for url, count in splits:\n",
    "            if len(url) < 3:\n",
    "                scores.update({'.'.join(['www', url[0]]) : count})\n",
    "            else:\n",
    "                scores.update({ '.'.join([url[0], url[1]]) : count})\n",
    "        ret = scores\n",
    "    else:\n",
    "        scores = Counter()\n",
    "        for url, count in ret:\n",
    "            scores.update({url: int(count)})\n",
    "        ret = scores\n",
    "    return ret\n",
    "\n",
    "def get_text(by=\"all\", minline=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Get the text from the files (by domain or year if desired).\n",
    "    \n",
    "    :param by: \"all\", \"domain\" or \"year\" the output to return.\n",
    "    :param minline: the minimum size of a line to be included in the output.\n",
    "    :return: [({year or domain}, textString)] if by is 'domain' or 'year', otherwise [textString].\n",
    "    \"\"\"\n",
    "    \n",
    "    text = []\n",
    "    form = range(RESULTS_START, RESULTS_LIMIT, RESULTS_STEP)\n",
    "    with open(auk_full_text) as fin:\n",
    "        for num in range(RESULTS_LIMIT):\n",
    "            if num in form:\n",
    "                line = next(fin)\n",
    "                split_line = str(line).split(\",\", 3)\n",
    "                if (len(split_line[3]) >= MINIMUM_WORD_LENGTH and set([split_line[3].find(x) for x in LINE_FILTER]) == {-1}):   \n",
    "                    if by == \"domain\": \n",
    "                        text.append((clean_domain(split_line[1]), split_line[3]))\n",
    "                    elif by == \"year\":\n",
    "                        text.append((split_line[0][1:5], split_line[3]))\n",
    "                    else:\n",
    "                        text.append(split_line[3])\n",
    "            else:\n",
    "                next(fin)                   \n",
    "    return text\n",
    "\n",
    "def get_text_tokens (minlen=MINIMUM_WORD_LENGTH) :\n",
    "    \"\"\"Get the data and tokenize the text.\n",
    "    \n",
    "    :param minlen: the minimum word size to be included in the list of words.\n",
    "    :return: a list of words included in the text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [x.lower() for x in word_tokenize(' '.join(get_text())) if len(x) > minlen]\n",
    "\n",
    "def get_tokens_domains(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Get tokens by domain.\n",
    "    \n",
    "    :param minlen: the minimum word size to be included in the list of words.\n",
    "    :return: a list of tuples with (domain, Counter).\n",
    "    \"\"\"\n",
    "    \n",
    "    return [(x[0], Counter([y for y in word_tokenize(x[1]) if len(y) > minlen])) for x in get_text(\"domain\")]\n",
    "\n",
    "def get_tokens_years(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Get tokens by year.\n",
    "    \n",
    "    :para minlen: the minimum word size to be included in the list of words.\n",
    "    :return: a list of tuples with (year, Counter).\n",
    "    \"\"\"\n",
    "    \n",
    "    return [(x[0], Counter([y for y in word_tokenize(x[1]) if len(y) > minlen])) for x in get_text(\"year\")]\n",
    "    \n",
    "\n",
    "def year(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Used by get_top_tokens_by to get the tokens by year.\"\"\"\n",
    "    return get_tokens_years(minlen)\n",
    "\n",
    "def domain(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Used by get_top_tokens_by to get tokens by domain.\"\"\"\n",
    "    return get_tokens_domains(minlen)\n",
    " \n",
    "def get_top_tokens(total=TOP_COUNT, minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Return the top tokens for the text.\"\"\"\n",
    "    return [(key, value) for key, value in Counter(get_text_tokens(minlen)).most_common(total)]\n",
    "\n",
    "def get_top_tokens_by(fun, total=TOP_COUNT, minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\" Get the top tokens by a function.\n",
    "    \n",
    "    :para fun: A function that returns a list of (key, Counter([tokenized_list])).\n",
    "    :para total: The number of top tokens to return for each key.\n",
    "    :para minlen: The minimum word length.\n",
    "    :return: list of minlen tokens by fun.\n",
    "    \"\"\"\n",
    "    \n",
    "    sep = dict()\n",
    "    tokens = fun(minlen)\n",
    "    sep = {k[0]: Counter() for k in tokens}\n",
    "    for key, value in tokens:\n",
    "        sep[key] += value\n",
    "    ret = [(key, val.most_common(total)) for key, val in sep.items()]\n",
    "    return (ret)\n",
    "\n",
    "\n",
    "def international(text):\n",
    "    \"\"\"Applies UTF-16 if possible.\n",
    "    \n",
    "    :param text: The text to decode (assumes UTF-8).\n",
    "    :return: UTF-32 or UTF-16 decoded string or else original string.\n",
    "    \"\"\"\n",
    "    \n",
    "    unicode = text.encode(\"utf-8\")\n",
    "    try:\n",
    "        ret = unicode.decode(\"UTF-32-LE\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            ret = unicode.decode(\"UTF-32-BE\")\n",
    "        except UnicodeDecodeError:\n",
    "            try: \n",
    "                ret = unicode.decode(\"UTF-16-LE\")\n",
    "            except UnicodeDecodeError:\n",
    "                try: \n",
    "                    ret = unicode.decode(\"UTF-16-BE\")\n",
    "                except UnicodeDecodeError:\n",
    "                    ret = unicode.decode(\"UTF-8\")\n",
    "            \n",
    "    return ret\n",
    "\n",
    "def write_output (stdout, results):\n",
    "    \"\"\" Writes results to file.\n",
    "    \n",
    "    :param stdout: Filepath for file.\n",
    "    :param results: A list of results.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"w\") as output:\n",
    "            for value in results:\n",
    "                output_file_write(str(value))\n",
    "    except:\n",
    "        print(\"Error writing the file.\")\n",
    "        \n",
    "def sentiment_scores(by=\"domain\"):\n",
    "    \"\"\" Calculates sentiment scores for a body of text.\n",
    "    \n",
    "    :param by: either \"year\" or \"domain\".\n",
    "    :return: a list of tuples with (year/domain, (\"neg\", score), (\"neu\", score) etc.).\n",
    "    \"\"\"\n",
    "    \n",
    "    sep = dict()\n",
    "    corpus = get_text(by)\n",
    "    sep = {k[0]: [] for k in corpus}\n",
    "    for key, value in corpus:\n",
    "        sep[key] += sent_tokenize(value)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    result = []\n",
    "    for a, b in sep.items():\n",
    "        scores = Counter({\"neg\": 0, \"pos\":0, \"neu\":0, \"compound\":0})\n",
    "        for c in b:\n",
    "            scores.update(sid.polarity_scores(c))\n",
    "        result += [(a, (\"neg\", scores['neg']/len(b)), (\"pos\", scores['neg']/len(b)), (\"neu\", scores['neu']/len(b)), (\"compound\", scores['compound']/len(b)))]\n",
    " \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Analysis of Domains\n",
    "\n",
    "Domains data provides basic information about what is crawled and how often. Change the variables in the following cell to manipulate the domain analysis. For example, you may want to exclude common domains (i.e. \"google\") or you may be interested in sub-domains such as ca.geocities.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_BY = 'name' # change to 'sub' if you want to include subdomains.\n",
    "DOMAIN_EXCLUDE = ['google', 'facebook', 'youtube', 'apple'] # add items to this list to exclude from the collection.\n",
    "DOMAIN_FIGURE_SIZE = [10, 4] # change the width and height of your graph plot ([wdth, hgt]).\n",
    "DOMAIN_RESULTS = 30 # the number of results to plot.\n",
    "DOMAIN_BAR_WIDTH = 0.35 # the width of the bars in the histogram.\n",
    "DOMAIN_Y_LABEL = \"Number of occurences.\" # The label for the y axis.\n",
    "DOMAIN_TITLE = \"Top domains by count.\" # The title of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the times a domain appears and count it.\n",
    "\n",
    "plt.rcParams['figure.figsize'] = DOMAIN_FIGURE_SIZE # Set the figure size for the graph.\n",
    "domains = get_domains(DOMAIN_BY).most_common(DOMAIN_RESULTS) \n",
    "vals = [x[1] for x in domains if x[0] not in DOMAIN_EXCLUDE]\n",
    "labs = [x[0] for x in domains if x[0] not in DOMAIN_EXCLUDE]\n",
    "ind = np.arange(len(vals))\n",
    "width = DOMAIN_BAR_WIDTH\n",
    "p1 = plt.bar(ind, vals, width)\n",
    "plt.ylabel(DOMAIN_Y_LABEL)\n",
    "plt.title(DOMAIN_TITLE)\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the domain derivative is relatively straightforward, there is not much else that we do with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Text Analysis\n",
    "\n",
    "The following set of functions use the [Natural Language Toolkit](https://www.nltk.org) Python library to search for the top most used words in the collection, as well as facilitate breaking it down by name or domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text by Year\n",
    "\n",
    "Change the variables in the following cell to manipulate the text analysis. For example, you may want to do your analysis by year, or you may be interested in analyzing text by domain (or both). You may also want to see more results than the default that we provide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_METHOD = 'year' # Choose \"year\", \"domain\", or \"all\".\n",
    "TEXT_OUTPUT_FILENAME = '' # Change this if want a custom filename, otherwise, OUTPUT_FILENAME will be used.\n",
    "TEXT_INTERNATIONAL = True # Change to False if you want only UTF-8 characters.\n",
    "TEXT_OUTPUT_SIZE = 10 # Change to increase or decrease the number of results shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of available years in the collection.\n",
    "\n",
    "year_range = set([x[0] for x in get_text(TEXT_METHOD)])\n",
    "print(year_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create separate lists with text files from individual years in this collection. You can add years (e.g. \"2019\") to `FILTERED_YEARS` in to limit your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and print the first n lines of text.\n",
    "\n",
    "filename = TEXT_OUTPUT_FILENAME if (TEXT_OUTPUT_FILENAME) else OUTPUT_FILENAME\n",
    "year_filter = FILTERED_YEARS if FILTERED_YEARS else year_range # Add or remove years for filter.\n",
    "year_results = [t[1] for t in get_text(TEXT_METHOD) if t[0] in list(year_filter)]\n",
    "     \n",
    "# Some of the text may be in an international font.\n",
    "\n",
    "for i in year_results[:TEXT_OUTPUT_SIZE]:\n",
    "    print (international(i)[:MAX_CHARACTERS] if (TEXT_INTERNATIONAL) else i[:MAX_CHARACTERS])\n",
    "\n",
    "## Removing the # on the following line will write the results to TEXT_OUTPUT_FILENAME if set, \n",
    "## or OUTPUT_FILENAME (set in the User Configuration section).\n",
    "\n",
    "#write_output(filename, year_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of available domains in the collection.\n",
    "\n",
    "domain_set = set([x[0] for x in get_text(\"domain\")])\n",
    "print(domain_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the variables in the following cell to restrict the text output to include only the text from the websites with those domain names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_SELECT_DOMAINS = [] # Restrict text extraction to these domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the given domain to a file and see how many results there are.\n",
    "\n",
    "domain_set = FILTERED_DOMAINS if FILTERED_DOMAINS else domain_set\n",
    "domain_set = TEXT_SELECT_DOMAINS if TEXT_SELECT_DOMAINS else domain_set\n",
    "domain_results = [t[1] for t in get_text(\"domain\") if t[0] in domain_set]\n",
    "print(\"****Total files****:\" + str(len(domain_results)) + \"\\n\")\n",
    "print(\"****Sample results****: \\n\")\n",
    "for samp in domain_results[:TEXT_OUTPUT_SIZE]:\n",
    "    if len(samp) > 4:\n",
    "        print(international(samp)[:MAX_CHARACTERS] + \" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Collection Characteristics\n",
    "\n",
    "Change the variables in the following cell to manipulate the analysis you'll be running to understand overall collection characteristics. In this case, they mostly affect the visualization that we generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERALL_OUTPUT_SIZE = 20 # Size of output to show.\n",
    "OVERALL_BAR_WIDTH = 0.35 # The size of the bars in the histogram.\n",
    "OVERALL_Y_LABEL = \"Number of occurences.\" # The Y axis label.\n",
    "OVERALL_TITLE = 'Top words by count.' # The plot title.\n",
    "OVERALL_SHOW_TOKENS = 3 # The number of top words to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the top words in the collection (regardless of year).\n",
    "\n",
    "tokens = get_top_tokens()[:OVERALL_OUTPUT_SIZE]\n",
    "vals = [x[1] for x in tokens if x[0] not in STOP_WORDS]\n",
    "labs = [x[0] for x in tokens if x[0] not in STOP_WORDS]\n",
    "ind = np.arange(len(vals))    # The x locations for the groups.\n",
    "width = OVERALL_BAR_WIDTH       # The width of the bars: can also be len(x) sequence.\n",
    "p1 = plt.bar(ind, vals, width)\n",
    "plt.ylabel(OVERALL_Y_LABEL)\n",
    "plt.title(OVERALL_TITLE)\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the top tokens, separated by year.\n",
    "\n",
    "get_top_tokens_by(year, OVERALL_SHOW_TOKENS)[0:OVERALL_OUTPUT_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of top tokens, separated by domain.\n",
    "\n",
    "get_top_tokens_by(domain, OVERALL_SHOW_TOKENS)[0:OVERALL_OUTPUT_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add or remove words to see their location in the output.\n",
    "\n",
    "DISPERSION_PLOT_WORDS = ['he', 'she'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dispersion plot, showing where the list of words appear in the text.\n",
    "\n",
    "text = get_text_tokens(1) # Need to have one to include words with fewer than 3 letters.\n",
    "dp(text, DISPERSION_PLOT_WORDS) # Uses the nltk dispersion plot library (dp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_Y_LABEL = \"Scores.\"\n",
    "SENTIMENT_TITLE = 'Scores by domain and sentiment.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentiment scores by domain and visualize them.\n",
    "\n",
    "sent = sentiment_scores()\n",
    "N = len(sent)\n",
    "neg = [x[1][1] for x in sent]\n",
    "pos = [x[2][1] for x in sent]\n",
    "neu = [x[3][1] for x in sent]\n",
    "labs = [x[0] for x in sent]\n",
    "ind = np.arange(N)    # The x locations for the groups.\n",
    "width = OVERALL_BAR_WIDTH\n",
    "p1 = plt.bar(ind, neg, width)\n",
    "p2 = plt.bar(ind, neu, width,\n",
    "             bottom=neg)\n",
    "p3 = plt.bar(ind, pos, width, bottom=neu)\n",
    "plt.ylabel(SENTIMENT_Y_LABEL)\n",
    "plt.title(SENTIMENT_TITLE)\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "plt.legend((p1[0], p2[0], p3[0]), ('Negative', 'Neutral', 'Positive'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_Y_LABEL2 = \"Scores.\"\n",
    "SENTIMENT_TITLE2 = 'Scores by year and sentiment.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentiment scores by year and visualize them.\n",
    "\n",
    "sent = sentiment_scores(\"year\")\n",
    "N = len(sent)\n",
    "neg = [x[1][1] for x in sent]\n",
    "pos = [x[2][1] for x in sent]\n",
    "neu = [x[3][1] for x in sent]\n",
    "labs = sorted([x[0] for x in sent])\n",
    "ind = np.arange(N)\n",
    "width = OVERALL_BAR_WIDTH\n",
    "p1 = plt.bar(ind, neg, width)\n",
    "p2 = plt.bar(ind, neu, width,\n",
    "             bottom=neg)\n",
    "p3 = plt.bar(ind, pos, width, bottom=neu)\n",
    "plt.ylabel(SENTIMENT_Y_LABEL2)\n",
    "plt.title(SENTIMENT_TITLE2)\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "plt.legend((p1[0], p2[0], p3[0]), ('Negative', 'Neutral', 'Positive'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis\n",
    "\n",
    "Sometimes it is useful to include information about the relationships among different websites. We recommend using [Gephi](https://gephi.org/) for graph visualization, but it is possible in this notebook using the Python networkx library as well.\n",
    "\n",
    "Change the variables in the following cell to manipulate the network analysis that you will be carrying out. Some of these relate to the size of the output, the labels on the graph, and also any domains you might want to exclude from your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_PLOT_FIGURE_SIZE = [10, 4] # Set the figure for the graph.\n",
    "NETWORK_EXCLUDE = [\"google.com\"] # Add / Remove urls if you want them out of the analysis.\n",
    "NETWORK_Y_LABEL = 'Degree'\n",
    "NETWORK_TITLE = 'Domains by degree.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree distribution of domains.\n",
    "\n",
    "plt.rcParams['figure.figsize'] = NETWORK_PLOT_FIGURE_SIZE # Set the figure size for the graph.\n",
    "graph = nx.read_gexf(auk_gephi) # Import the graph.\n",
    "g_nodes = zip([x[1] for x in graph.nodes('label')], [x[1] for x in graph.nodes('Degree')])\n",
    "gs = sorted([x for x in g_nodes], key=lambda s: s[1], reverse=True)\n",
    "vals = [x[1] for x in gs if x[0] not in NETWORK_EXCLUDE][:10]\n",
    "labs = [x[0] for x in gs if x[0] not in NETWORK_EXCLUDE][:10]\n",
    "ind = np.arange(len(vals))\n",
    "width = OVERALL_BAR_WIDTH\n",
    "p1 = plt.bar(ind, vals, width)\n",
    "plt.ylabel(NETWORK_Y_LABEL)\n",
    "plt.title(NETWORK_TITLE)\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the variables in the following cell to manipulate the visualization that you're about to generate. Do you want the network diagram to be bigger or smaller? Do nodes need to shrink or grow? Are the labels too big or too small? You may need to experiment with the values a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_GRAPH_FIGURE_SIZE = [25, 25] # Change the size of the plot.\n",
    "NETWORK_NODE_SIZE = 100 # increase or decrease the node size for the graph.\n",
    "NETWORK_FONT_SIZE = 10 # increase or decrease the font size for the graph.\n",
    "NETWORK_SHOW_LABELS = True # change to False if you do not want to see the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network graph.\n",
    "\n",
    "plt.rcParams['figure.figsize'] = NETWORK_GRAPH_FIGURE_SIZE\n",
    "\n",
    "# Convert rgb values to between 0 & 1 and put them into a numpy array.\n",
    "\n",
    "rgbs = zip([x[1]/255 for x in graph.nodes('r')], \n",
    "           [x[1]/255 for x in graph.nodes('g')], \n",
    "           [x[1]/255 for x in graph.nodes('b')])\n",
    "colormap = [np.array(x) for x in rgbs]\n",
    "\n",
    "# Labels.\n",
    "\n",
    "mapping = {x[0]: x[1] for x in graph.nodes('label')}\n",
    "\n",
    "# Use Archive Unleashed Clouds positions (saves on load time).\n",
    "\n",
    "zippos = zip(graph.nodes, [x[1] for x in graph.nodes('x')], [x[1] for x in graph.nodes('y')])\n",
    "positions = {x[0]: np.array([x[1],x[2]]) for x in zippos}\n",
    "\n",
    "# Node sizes based on degree.\n",
    "\n",
    "size = np.array([x[1] * NETWORK_NODE_SIZE for x in graph.nodes('size')])\n",
    "\n",
    "# Draw the graph.\n",
    "\n",
    "nx.draw(graph, pos=positions, show_labels=NETWORK_SHOW_LABELS, \n",
    "        labels=mapping, font_size=NETWORK_FONT_SIZE, \n",
    "        node_size=size, node_color=colormap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the variables in the following cell to manipulate the visualization that you are about to generate. What node do you want the diagram to focus on? The biggest one? Or a specific one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_EGO_NODE_ORDINAL = 0 # Selects the largest node (by degree). Use `1` for 2nd largest,`2` for third largest etc.\n",
    "NETWORK_EGO_NODE_SPECIFIC = '' # Change to a non-signed node id (eg. 'n20') if you want to select a particular node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ego network for a particular node.\n",
    "\n",
    "largest_node = sorted(graph.nodes('Degree'), \n",
    "                      key=lambda s: s[1], \n",
    "                      reverse=True)[NETWORK_EGO_NODE_ORDINAL][0] # [1][0] is second largest, etc.\n",
    "node = NETWORK_EGO_NODE_SPECIFIC if (NETWORK_EGO_NODE_SPECIFIC) else largest_node\n",
    "neigh = graph.subgraph(graph.neighbors(node))\n",
    "\n",
    "# Convert rgb values to between 0 & 1 and put them into a numpy array.\n",
    "\n",
    "rgbs = zip([x[1]/255 for x in neigh.nodes('r')], \n",
    "           [x[1]/255 for x in neigh.nodes('g')], \n",
    "           [x[1]/255 for x in neigh.nodes('b')])\n",
    "colormap = [np.array(x) for x in rgbs]\n",
    "\n",
    "# Labels.\n",
    "\n",
    "mapping = {x[0]: x[1] for x in neigh.nodes('label')}\n",
    "\n",
    "# Use Archive Unleashed Clouds positions (saves on load time).\n",
    "\n",
    "zippos = zip(neigh.nodes, [x[1] for x in neigh.nodes('x')], [x[1] for x in neigh.nodes('y')])\n",
    "positions = {x[0]: np.array([x[1],x[2]]) for x in zippos}\n",
    "\n",
    "# Node sizes based on degree.\n",
    "\n",
    "size = np.array([x[1] * NETWORK_NODE_SIZE for x in neigh.nodes('size')])\n",
    "nx.draw(neigh, pos=positions, show_labels=NETWORK_SHOW_LABELS, \n",
    "        labels=mapping, font_size=NETWORK_FONT_SIZE, \n",
    "        node_size=size, node_color=colormap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "- Archives Unleashed Project. (2018). Archives Unleashed Toolkit (Version 0.17.0). Apache License, Version 2.0.\n",
    "- Aric A. Hagberg, Daniel A. Schult and Pieter J. Swart, “Exploring network structure, dynamics, and function using NetworkX”, in Proceedings of the 7th Python in Science Conference (SciPy2008), Gäel Varoquaux, Travis Vaught, and Jarrod Millman (Eds), (Pasadena, CA USA), pp. 11–15, Aug 2008.\n",
    "- Bird, Steven, Edward Loper and Ewan Klein (2009), *Natural Language Processing with Python*. O’Reilly Media Inc.\n",
    "- University of Victoria Libraries, B.C. Teachers' Labour Dispute (2014), Archive-It Collection 4867, https://archive-it.org/collections/4867."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
