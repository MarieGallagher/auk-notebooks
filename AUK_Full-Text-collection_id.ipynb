{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/archivesunleashed/archivesunleashed.org/master/themes/hugo-material-docs/static/images/cropped-logo.png\" height=\"200px\" width=\"500px\">\n",
    "\n",
    "# Welcome\n",
    "\n",
    "Welcome to the Archives Unleashed Cloud Visualization Demo in Jupyter Notebook for your collection. This demonstration takes the main derivatives from the Cloud and uses Python to analyze and produce information about your collection.\n",
    "\n",
    "This product is in beta, so if you encounter any issues, please post an [issue in our Github repository](https://github.com/archivesunleashed/auk/issues) to let us know about any bugs you encountered or features you would like to see included.\n",
    "\n",
    "If you have some basic Python coding experience, you can change the code we provided to suit your own needs.\n",
    "\n",
    "Unfortunately, we cannot support code that you produced yourself. We recommend that you use `File > Make a Copy` first before changing the code in the repository. That way, you can always return to the basic visualizations we have offered here. Of course, you can also just re-download the Jupyter Notebook file from your Archives Unleashed Cloud account.\n",
    "\n",
    "### How Jupyter Notebooks Work:\n",
    "\n",
    "If you have no previous experience of Jupyter Notebooks, the most important thing to understand is that that <Shift><Enter/Return> will run the python code inside a window and output it to the site.\n",
    "    \n",
    "The window titled `# RUN THIS FIRST` should be the first place you go. This will import all the libraries and set basic variables (e.g. where your derivative files are located) for the notebook. After that, everything else should be able to run on its own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS FIRST\n",
    "\n",
    "# This Window will set up all the necessary libraries and dependencies\n",
    "# for your Collection.\n",
    "coll_id = \"4656\"\n",
    "auk_fp = \"data/\"\n",
    "auk_full_text = auk_fp + coll_id + \"-fulltext.txt\"\n",
    "auk_gephi = auk_fp + coll_id + \"-gephi.gexf\"\n",
    "auk_graphml = auk_fp + coll_id + \"-gephi.grapml\"\n",
    "auk_domains = auk_fp + coll_id + \"-fullurls.txt\"\n",
    "auk_filtered_text = auk_fp + coll_id + \"-filtered_text.zip\"\n",
    "\n",
    "# The following script will attempt to install the necessary dependencies\n",
    "# for the visualisations. You may prefer to install these on your\n",
    "# own in the command line.\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "try:  # a library for manipulating column data.\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    !{sys.executable} -m pip install pandas  \n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt # a library for Plotting\n",
    "except ImportError:\n",
    "    !{sys.executable} -m pip install matplotlib\n",
    "\n",
    "try:\n",
    "    import numpy as np # a library for complex mathematics\n",
    "except ImportError:\n",
    "    !{sys.executable} -m pip install numpy\n",
    "    \n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.draw.dispersion import dispersion_plot as dp\n",
    "except ImportError:\n",
    "    !{sys.executable} -m pip install nltk\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "The following set of functions use the [Natural Language Toolkit](https://www.nltk.org) Python library to search for the top most used words in the collection, as well as facilitate breaking it down by name or domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can change the value of `top` to get more results. \n",
    "top = 30\n",
    "\n",
    "def clean_domain(s):\n",
    "    stop_words = [\"com\", \"org\", \"net\", \"edu\"]\n",
    "    ret = \"\"\n",
    "    dom = s.split(\".\")\n",
    "    if len(dom) <3:\n",
    "        ret = dom[0]\n",
    "    elif dom[-2] in stop_words:\n",
    "        ret = dom[-3]\n",
    "    else:\n",
    "        ret = dom[1]\n",
    "    return ret\n",
    "\n",
    "def get_textfile () :\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            tokens += word_tokenize(str(line).split(\",\")[3])\n",
    "    return tokens\n",
    "\n",
    "def get_text_domains():\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            split_line = str(line).split(',')\n",
    "            tokens.append((clean_domain(split_line[1]), split_line[3]))\n",
    "    return tokens\n",
    "\n",
    "def get_text_years():\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            split_line = str(line).split(',')\n",
    "            tokens.append((split_line[0][1:5], split_line[3]))\n",
    "    return tokens\n",
    "\n",
    "def get_top_tokens(total=20):\n",
    "    tokens = get_textfile()\n",
    "    tokens = [(value, key) for key, value in Counter(tokens).items()]\n",
    "    tokens = list(filter(lambda x : len(x[1]) > 3, tokens))\n",
    "    tokens.sort(reverse=True)\n",
    "    return(tokens[0:total])\n",
    "\n",
    "def get_top_tokens_by_year(total=20):\n",
    "    tokens = get_text_years()\n",
    "    sep = {key: \"\" for key, value in tokens}\n",
    "    for year, text in tokens:\n",
    "        sep[str(year)] = sep[str(year)] + \" \" + text\n",
    "    ret = [(key, Counter(word_tokenize(val)).most_common(total)) for key, val in sep.items()]\n",
    "    return (ret)\n",
    "\n",
    "def get_top_tokens_by_domain(total=20):\n",
    "    tokens = get_text_domains()\n",
    "    sep = {key: \"\" for key, value in tokens}\n",
    "    for domain, text in tokens:\n",
    "        sep[str(domain)] = sep[str(domain)] + \" \" + text\n",
    "    ret = [(key, Counter(word_tokenize(val)).most_common(total)) for key, val in sep.items()]\n",
    "    return (ret)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have saved the above functions, you can now use them in a variety of ways. \n",
    "\n",
    "### Text by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2009', '2014', '2016'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the set of available years in the collection \n",
    "set([x[0] for x in get_text_years()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create separate lists with text files from individual years in this collection. The example below selects all items from the year 2016. You may need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_results = [t[1] for t in get_text_years() if t[0] == \"2016\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[')\\n',\n",
       " ')\\n',\n",
       " ')\\n',\n",
       " '404 Not Found Not Found The requested URL /thumbs/c3e0677be2f6.jpg was not found on this server.)\\n',\n",
       " '404 Not Found Not Found The requested URL /thumbs/6a5e80c11f3d.jpg was not found on this server.)\\n',\n",
       " '404 Not Found Not Found The requested URL /thumbs/eb55c88d287c.png was not found on this server.)\\n',\n",
       " \"�\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x03�}kS#9��g&b���=��\\x1fe\\x1b�G�>@C\\x0fs�nNC���� BU%�Ք�jJU���#�g�\\x13q��_r3%��/ �\\u05ec�\\x19�R��L�2�T����~�?���\\x01����19��w|�Or�R���~�����P+�5r�S�[��:�.�\\x0e>�H�\\x1f\\x04^�T���*^U���+�}*���]+ٮ�Y� ��Ο~x�i�z`;�3���l6Kט''3��^���y�\\x01+9�U����\\x7f\",\n",
       " '302 Found Found The document has moved here.)\\n',\n",
       " 'Save.ca - Coupons and Deals For All of Canada - Save.ca - ExternalWidget Flyers WEEKLY FLYERS LOADING \\xa0&nbspFLYER LOADING \\xa0&nbspFLYER LOADING \\xa0&nbspFLYER LOADING \\xa0&nbspFLYER LOADING \\xa0&nbspFLYER LOADING \\xa0&nbspFLYER LOADING \\xa0&nbspFLYER LOADING \\xa0&nbspFLYER LOADING \\xa0&nbspFLYER LOADING \\xa0&nbspFLYER)\\n',\n",
       " 'JobPosting - schema.org schema.org Documentation Schemas Home JobPosting Thing > Intangible > JobPosting A listing that describes a job opening in a certain organization. Usage: Between 10']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first ten results from the year specified above\n",
    "year_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you may now want to export this file so you can work with it. \n",
    "# this will appear in the directory that this notebook is in\n",
    "# you may want to change the output path\n",
    "\n",
    "with open(\"results-2016.txt\", \"w\") as output_file:\n",
    "    for value in year_results:\n",
    "        output_file.write(str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuweather',\n",
       " 'bcdailybuzz',\n",
       " 'blackpress',\n",
       " 'facebook',\n",
       " 'google',\n",
       " 'googleapis',\n",
       " 'googlesyndication',\n",
       " 'issuu',\n",
       " 'nanaimodailynews',\n",
       " 'save',\n",
       " 'schema'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the set of available domains in the collection \n",
    "set([x[0] for x in get_text_domains()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9835"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract only the given domain to a file and see how many results there are\n",
    "\n",
    "domain_results = [t[1] for t in get_text_domains() if t[0] == \"nanaimodailynews\"]\n",
    "len(domain_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Nanaimo Daily News - Nanaimo Daily News Real Estate e-Edition Obits 27° Sunny Home Our Team Contact Us Advertising Info News BC News Sports BC Sports BC Games NFL Business BC Business Entertainment BC Arts & Entertainment What's on Guide Our Town Opinion BC Opinions Letters Web Poll Driveway World Canada / World Sports Vancouver \\xa0 Classifieds Browse Classifieds BC Jobs Connect with Us \\xa0 Goodbye posted Jan 29\",\n",
       " \"�\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x03�}kS#9��g&b���=��\\x1fe\\x1b�G�>@C\\x0fs�nNC���� BU%�Ք�jJU���#�g�\\x13q��_r3%��/ �\\u05ec�\\x19�R��L�2�T����~�?���\\x01����19��w|�Or�R���~�����P+�5r�S�[��:�.�\\x0e>�H�\\x1f\\x04^�T���*^U���+�}*���]+ٮ�Y� ��Ο~x�i�z`;�3���l6Kט''3��^���y�\\x01+9�U����\\x7f\",\n",
       " 'Nanaimo Daily News - Local News',\n",
       " '�\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x03�Z�n\\x1b�\\x15��\\x00y�1�f���$�(q)�E|k\\x058\\x17$\\x0e��0���Yr��yw(J \\x04\\x14}����E�$O�o�����d�r�ݝ9�̹|s�e�\\x0f������~|A\\x16\"K�|��T^IJ�����.F:�\\x05����L3&(�\\x16����u�\"�\\x1fw[3\\x0b!J��[�Y���/��ϊ����)뒨�\\x05��v�b��9\\x1bD����ll$\\x08.R��/��{5\\x1d�{5\\\\G\\x15/\\x05��h֕�\\'�!=����(�)�%������0�a=<{�d��p\\x1c<\\x0e��!�x\\x1e���\\'ӡ\\x16�Dk��J�$�\\x1e����\\x06$�\\x17\\x03R�4\\x1f\\x10Z�)\\x13\\x03R�g',\n",
       " \"Nanaimo Daily News - eEdition (Real Estate) Real Estate e-Edition Obits 27° Sunny Home Our Team Contact Us Advertising Info News BC News Sports BC Sports BC Games NFL Business BC Business Entertainment BC Arts & Entertainment What's on Guide Our Town Opinion BC Opinions Letters Web Poll Driveway World Canada / World Sports Vancouver \\xa0 Classifieds Browse Classifieds BC Jobs Real Estate Guide\"]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first five results from the year specified above\n",
    "domain_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you may now want to export this file so you can work with the text of one domain. \n",
    "# this will appear in the directory that this notebook is in\n",
    "# you may want to change the output path\n",
    "\n",
    "with open(\"results-domain.txt\", \"w\") as output_file:\n",
    "    for value in domain_results:\n",
    "        output_file.write(str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Collection Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-208e7d0f47ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get a list of the top words in the collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# (regardless of year).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_top_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-138-55a6c45466b4>\u001b[0m in \u001b[0;36mget_top_tokens\u001b[0;34m(total)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_textfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# Get a list of the top words in the collection\n",
    "# (regardless of year).\n",
    "get_top_tokens(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of the top tokens, separated by year.\n",
    "get_top_tokens_by_year(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_top_tokens_by_domain(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dispersion plot, showing where the list of words appear\n",
    "# in the text.\n",
    "text = get_textfile()\n",
    "dp(text, [\"he\", \"she\"]) # uses the nltk dispersion plot library (dp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "Bird, Steven, Edward Loper and Ewan Klein (2009), *Natural Language       Processing with Python*. O’Reilly Media Inc.\n",
    "\n",
    "Archives Unleashed Project. (2018). Archives Unleashed Toolkit (Version 0.17.0). Apache License, Version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
