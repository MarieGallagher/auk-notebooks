{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/archivesunleashed/archivesunleashed.org/master/themes/hugo-material-docs/static/images/logo-square.png\" height=\"100px\" width=\"100px\">\n",
    "\n",
    "\n",
    "# Welcome\n",
    "\n",
    "Welcome to the Archives Unleashed Cloud Visualization Demo in Jupyter Notebook for your collection. This demonstration takes the main derivatives from the Cloud and uses Python to analyze and produce information about your collection.\n",
    "\n",
    "This product is in beta, so if you encounter any issues, please post an [issue in our Github repository](https://github.com/archivesunleashed/auk/issues) to let us know about any bugs you encountered or features you would like to see included.\n",
    "\n",
    "If you have some basic Python coding experience, you can change the code we provided to suit your own needs.\n",
    "\n",
    "Unfortunately, we cannot support code that you produced yourself. We recommend that you use `File > Make a Copy` first before changing the code in the repository. That way, you can always return to the basic visualizations we have offered here. Of course, you can also just re-download the Jupyter Notebook file from your Archives Unleashed Cloud account.\n",
    "\n",
    "### How Jupyter Notebooks Work:\n",
    "\n",
    "If you have no previous experience of Jupyter Notebooks, the most important thing to understand is that that <Shift><Enter/Return> will run the python code inside a window and output it to the site.\n",
    "    \n",
    "The window titled `# RUN THIS FIRST` should be the first place you go. This will import all the libraries and set basic variables (e.g. where your derivative files are located) for the notebook. After that, everything else should be able to run on its own.\n",
    "\n",
    "If you just want to see the results for your collection, use `Cell > Run All`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports from sys\n",
    "\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "coll_id = \"4656\"\n",
    "auk_fp = \"./data/\"\n",
    "auk_full_text = auk_fp + coll_id + \"-fulltext.txt\"\n",
    "auk_gephi = auk_fp + coll_id + \"-gephi.gexf\"\n",
    "auk_graphml = auk_fp + coll_id + \"-gephi.grapml\"\n",
    "auk_domains = auk_fp + coll_id + \"-fullurls.txt\"\n",
    "auk_filtered_text = auk_fp + coll_id + \"-filtered_text.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    output = \"This script will check if you have all required dependencies:\\n\\n\"\n",
    "    try:\n",
    "        import matplotlib.pyplot\n",
    "        output += \"Matplotlib OK.\\n\"\n",
    "    except ImportError:\n",
    "        output += \"ERROR MATPLOTLIB required: Matplotlib is required to show graph visualizations.\\n\"\n",
    "        output += \"                           Try `pip install -u matplotlib` to install.\\n\"\n",
    "    try:\n",
    "        import numpy\n",
    "        output += \"Numpy OK.\\n\"\n",
    "    except ImportError:\n",
    "        output += \"WARN NUMPY MISSING: You will not be able to use some math features.\\n\"\n",
    "        output += \"                    Try `pip install -u numpy` to install.\\n\"\n",
    "    try:\n",
    "        import pandas\n",
    "        output += \"Pandas OK.\\n\"\n",
    "    except ImportError:\n",
    "        output += \"WARN PANDAS MISSING: This means you cannot access dataframes.\\n\"\n",
    "        output += \"                     Try `pip install -u pandas` to install.\\n\"\n",
    "    try: \n",
    "        from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "        from nltk.draw.dispersion import dispersion_plot as dp\n",
    "        nltk.download('vader_lexicon')\n",
    "        output += \"Nltk OK.\\n\"\n",
    "    except ImportError:\n",
    "        output += \"WARN NLTK MISSING: This means you cannot do text analysis.\\n\"\n",
    "        output += \"                   Try `pip install -u nltk` to install.\\n\"\n",
    "    try:\n",
    "        open(auk_full_text)\n",
    "        output += \"Full text OK.\\n\"\n",
    "    except NameError:\n",
    "        output += \"ERROR No File Variables: Looks like you forgot to run the first window.\\n\"\n",
    "        output += \"                         Use <SHIFT><RETURN/ENTER> on the first code window\\n\"\n",
    "    except FileNotFoundError:\n",
    "        output += \"ERROR No File Access: Could not open the full-text file.\\n\"\n",
    "        output += \"                      Did you remember to include the downloads in the same folder?\\n\"\n",
    "    try:\n",
    "        open(auk_gephi)\n",
    "        output += \"Gephi OK.\\n\"\n",
    "    except NameError:\n",
    "        output += \"ERROR No File Variables: Looks like you forgot to run the first window.\\n\"\n",
    "        output += \"                         Use <SHIFT><RETURN/ENTER> on the first code window\\n\"\n",
    "    except FileNotFoundError:\n",
    "        output += \"ERROR No File Access: Could not open the gephi file.\\n\"\n",
    "        output += \"                      Did you remember to include the downloads in the same folder?\\n\"\n",
    "    try:\n",
    "        open(auk_domains)\n",
    "        output += \"Domains OK.\\n\"\n",
    "    except NameError:\n",
    "        output += \"ERROR No File Variables: Looks like you forgot to run the first window.\\n\"\n",
    "        output += \"                         Use <SHIFT><RETURN/ENTER> on the first code window\\n\"\n",
    "    except FileNotFoundError:\n",
    "        output += \"ERROR No File Access: Could not open the domains file.\\n\"\n",
    "        output += \"                      Did you remember to include the downloads in the same folder?\\n\"\n",
    "    return (output)\n",
    "\n",
    "print(sanity_check())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Text Analysis\n",
    "\n",
    "The following set of functions use the [Natural Language Toolkit](https://www.nltk.org) Python library to search for the top most used words in the collection, as well as facilitate breaking it down by name or domain.\n",
    "\n",
    "Set the variables below if you wish to make some changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  CONSTANTS / CONFIGURATION\n",
    "#\n",
    "# If you wish to fine tune the outputs, you may change the following:\n",
    "#\n",
    "# maximum number of words to show in output.\n",
    "# Jupyter will create an output error if the number is too high.\n",
    "TOP_COUNT = 30 \n",
    "\n",
    "# Domain suffixes to check non-U.S. domains.\n",
    "# so that (e.g.) www.google.co.uk will become \"google\"\n",
    "STOP_DOMAINS = [\"co\", \"org\", \"net\", \"edu\"] # domain suffixes to remove\n",
    "\n",
    "# minimum number of characters for a word to be included in a corpus\n",
    "MINIMUM_WORD_LENGTH = 3 # eliminates \"it\", \"I\", \"be\" etc.\n",
    "\n",
    "# list of substrings to filter a text line, if desired\n",
    "LINE_FILTER = ['404 Not Found']\n",
    "\n",
    "\n",
    "## Toolkit imports\n",
    "import matplotlib.pyplot as pp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.draw.dispersion import dispersion_plot as dp\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def clean_domain(s):\n",
    "    \"\"\"Extracts the name from the domain (e.g. 'www.google.com' becomes 'google').\"\"\"\n",
    "    ret = \"\"\n",
    "    dom = s.split(\".\")\n",
    "    if len(dom) <3: # x.com is always x\n",
    "        ret = dom[0]\n",
    "    elif dom[-2] in STOP_DOMAINS: # www.x.co.uk should be x\n",
    "        ret = dom[-3]\n",
    "    else:\n",
    "        ret = dom[1]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_text(by=\"all\", minline=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Get just the text from the files.\"\"\"\n",
    "    text = []\n",
    "    with open(auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            split_line = str(line).split(\",\")\n",
    "            if (len(split_line[3]) >= 3 and set([split_line[3].find(x) for x in LINE_FILTER]) == {-1}):   \n",
    "                if by == \"domain\":    \n",
    "                    text.append((clean_domain(split_line[1]), split_line[3]))\n",
    "                elif by == \"year\":\n",
    "                    text.append((split_line[0][1:5], split_line[3]))\n",
    "                else:\n",
    "                    text.append(split_line[3])\n",
    "    return text\n",
    "\n",
    "def get_text_tokens (minlen=MINIMUM_WORD_LENGTH) :\n",
    "    \"\"\"Get the data and tokenize the text.\"\"\"\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            split_line = str(line).split(\",\")\n",
    "            # remove lines that are smaller than 3 chars or in the FILTER list.\n",
    "            if (len(split_line[3]) >= 3 and set([split_line[3].find(x) for x in LINE_FILTER]) == {-1}):\n",
    "                tokens += word_tokenize(str(line).split(\",\")[3])\n",
    "    tokens = [x for x in tokens if len(x) > minlen]\n",
    "    return tokens\n",
    "\n",
    "def get_tokens_domains(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Get tokens by domain\"\"\"\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            split_line = str(line).split(',')\n",
    "            if (len(split_line[3]) >= 3 and set([split_line[3].find(x) for x in LINE_FILTER]) == {-1}):\n",
    "                tokens.append((clean_domain(split_line[1]), Counter([x for x in word_tokenize(str(split_line[3])) if len(x) > minlen])))\n",
    "    return tokens\n",
    "\n",
    "def get_tokens_years(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Get tokens by year.\"\"\"\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            split_line = str(line).split(',')\n",
    "            if (len(split_line[3]) >= 3 and set([split_line[3].find(x) for x in LINE_FILTER]) == {-1}):\n",
    "                tokens.append((split_line[0][1:5], Counter([x for x in word_tokenize(str(split_line[3])) if len(x) > minlen])))\n",
    "    return tokens\n",
    "\n",
    "def year(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Used by get_top_tokens_by to get the tokens by year.\"\"\"\n",
    "    return get_tokens_years(minlen)\n",
    "\n",
    "def domain(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Used by get_top_tokens_by to get tokens by domain.\"\"\"\n",
    "    return get_tokens_domains(minlen)\n",
    " \n",
    "def get_top_tokens(total=TOP_COUNT, minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Return the top tokens for the text.\"\"\"\n",
    "    return [(key, value) for key, value in Counter(get_text_tokens(minlen)).most_common(total)]\n",
    "\n",
    "def get_top_tokens_by(fun, total=TOP_COUNT, minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\" Get the top tokens by a function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fun: A function that returns a list of (key, Counter([tokenized_list]))\n",
    "    total: The number of top tokens to return for each key.\n",
    "    minlen: The minimum word length.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret: list of minlen tokens by fun.\n",
    "    \"\"\"\n",
    "    \n",
    "    sep = dict()\n",
    "    tokens = fun(minlen)\n",
    "    sep = {k[0]: Counter() for k in tokens}\n",
    "    for key, value in tokens:\n",
    "        sep[key] += value\n",
    "    ret = [(key, val.most_common(total)) for key, val in sep.items()]\n",
    "    return (ret)\n",
    "\n",
    "# applies utf-16 encoding to text, if possible\n",
    "def international(text):\n",
    "    unicode = text.encode(\"utf-8\")\n",
    "    try:\n",
    "        ret = unicode.decode(\"UTF-32-LE\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            ret = unicode.decode(\"UTF-32-BE\")\n",
    "        except UnicodeDecodeError:\n",
    "            try: \n",
    "                ret = unicode.decode(\"UTF-16-LE\")\n",
    "            except UnicodeDecodeError:\n",
    "                try: \n",
    "                    ret = unicode.decode(\"UTF-16-BE\")\n",
    "                except UnicodeDecodeError:\n",
    "                    logging.warning(\"Could not convert text to UTF-16 or UTF-32. Restoring UTF-8\")\n",
    "                    ret = unicode.decode(\"UTF-8\")\n",
    "            \n",
    "    return ret\n",
    "\n",
    "# writes results to stdout\n",
    "def write_output (stdout, results):\n",
    "    try:\n",
    "        with open(filename, \"w\") as output:\n",
    "            for value in results:\n",
    "                output_file_write(str(value))\n",
    "    except:\n",
    "        print(\"Error writing the file.\")\n",
    "        \n",
    "def sentiment_scores(by=\"domain\"):\n",
    "    sep = dict()\n",
    "    corpus = get_text(by)\n",
    "    sep = {k[0]: [] for k in corpus}\n",
    "    for key, value in corpus:\n",
    "        sep[key] += sent_tokenize(value)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    result = []\n",
    "    for a, b in sep.items():\n",
    "        scores = Counter({\"neg\": 0, \"pos\":0, \"neu\":0, \"compound\":0})\n",
    "        for c in b:\n",
    "            scores.update(sid.polarity_scores(c))\n",
    "        result += [(a, (\"neg\", scores['neg']/len(b)), (\"pos\", scores['neg']/len(b)), (\"neu\", scores['neu']/len(b)), (\"compound\", scores['compound']/len(b)))]\n",
    "      #  [(key, sid.polarity_scores(text)) for key, text in sep.items()]\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have saved the above functions, you can now use them in a variety of ways. \n",
    "\n",
    "### Text by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"year\" # choose \"year\", \"domain\" or \"all\"\n",
    "\n",
    "# Get the set of available years in the collection \n",
    "year_range = set([x[0] for x in get_text(method)])\n",
    "print(year_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create separate lists with text files from individual years in this collection. You can add years (e.g. \"2019\") to `filtered_range` to limit your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if you want a different filename.\n",
    "OUTPUT_FILENAME = \"./filtered_text.txt\" # filename if you want to output to another file.\n",
    "\n",
    "# The years to include in the analysis. If empty, you will get all available years.\n",
    "FILTERED_YEARS = [] # e.g. ['2015', '2016', '2019']\n",
    "\n",
    "year_filter = FILTERED_YEARS if FILTERED_YEARS else year_range # add or remove years for filter\n",
    "year_results = [t[1] for t in get_text(\"year\") if t[0] in list(year_filter)]\n",
    "    \n",
    "# Some of the text may be in an international font.\n",
    "for i in year_results[:10]:\n",
    "    print(international(i))\n",
    "\n",
    "## Commenting out the following will write the results to a `output_filename\n",
    "\n",
    "#write_output(output_filename, year_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of available domains in the collection \n",
    "domain_set = set([x[0] for x in get_text(\"domain\")])\n",
    "print(domain_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the given domain to a file and see how many results there are\n",
    "filtered_domains = [] # e.g [\"google\", \"apple\", \"facebook\"]\n",
    "domain_set = filtered_domains if filtered_domains else domain_set\n",
    "domain_results = [t[1] for t in get_text(\"domain\") if t[0] in domain_set]\n",
    "print(\"****Total files****:\" + str(len(domain_results)) + \"\\n\")\n",
    "print(\"****Sample results****: \\n\")\n",
    "for samp in domain_results[:10]:\n",
    "    if len(samp) > 4:\n",
    "        print(international(samp) + \" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Collection Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the top words in the collection\n",
    "# (regardless of year).\n",
    "get_top_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the top tokens, separated by year.\n",
    "get_top_tokens_by(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of top tokens, separated by domain.\n",
    "get_top_tokens_by(domain, top, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dispersion plot, showing where the list of words appear\n",
    "# in the text.\n",
    "text = get_text_tokens(1)\n",
    "dp(text, [\"he\", \"she\"]) # uses the nltk dispersion plot library (dp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sentiment_scores()\n",
    "N = len(sent)\n",
    "neg = [x[1][1] for x in sent]\n",
    "pos = [x[2][1] for x in sent]\n",
    "neu = [x[3][1] for x in sent]\n",
    "labs = [x[0] for x in sent]\n",
    "\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = pp.bar(ind, neg, width)\n",
    "p2 = pp.bar(ind, neu, width,\n",
    "             bottom=neg)\n",
    "p3 = pp.bar(ind, pos, width, bottom=neu)\n",
    "\n",
    "pp.ylabel('Scores')\n",
    "pp.title('Scores by domain and sentiment')\n",
    "pp.xticks(ind, labs, rotation='vertical')\n",
    "pp.legend((p1[0], p2[0], p3[0]), ('Negative', 'Neutral', 'Positive'))\n",
    "\n",
    "pp.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "Bird, Steven, Edward Loper and Ewan Klein (2009), *Natural Language       Processing with Python*. O’Reilly Media Inc.\n",
    "\n",
    "Archives Unleashed Project. (2018). Archives Unleashed Toolkit (Version 0.17.0). Apache License, Version 2.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
