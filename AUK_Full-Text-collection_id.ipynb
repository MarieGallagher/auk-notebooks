{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/archivesunleashed/archivesunleashed.org/master/themes/hugo-material-docs/static/images/logo-square.png\" height=\"100px\" width=\"100px\">\n",
    "\n",
    "\n",
    "# Welcome\n",
    "\n",
    "Welcome to the Archives Unleashed Cloud Visualization Demo in Jupyter Notebook for your collection. This demonstration takes the main derivatives from the Cloud and uses Python to analyze and produce information about your collection.\n",
    "\n",
    "This product is in beta, so if you encounter any issues, please post an [issue in our Github repository](https://github.com/archivesunleashed/auk/issues) to let us know about any bugs you encountered or features you would like to see included.\n",
    "\n",
    "If you have some basic Python coding experience, you can change the code we provided to suit your own needs.\n",
    "\n",
    "Unfortunately, we cannot support code that you produced yourself. We recommend that you use `File > Make a Copy` first before changing the code in the repository. That way, you can always return to the basic visualizations we have offered here. Of course, you can also just re-download the Jupyter Notebook file from your Archives Unleashed Cloud account.\n",
    "\n",
    "### How Jupyter Notebooks Work:\n",
    "\n",
    "If you have no previous experience of Jupyter Notebooks, the most important thing to understand is that that <Shift><Enter/Return> will run the python code inside a window and output it to the site.\n",
    "    \n",
    "The window titled `# RUN THIS FIRST` should be the first place you go. This will import all the libraries and set basic variables (e.g. where your derivative files are located) for the notebook. After that, everything else should be able to run on its own.\n",
    "\n",
    "If you just want to see the results for your collection, use `Cell > Run All`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports from sys\n",
    "\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "coll_id = \"4656\"\n",
    "auk_fp = \"./data/\"\n",
    "auk_full_text = auk_fp + coll_id + \"-fulltext.txt\"\n",
    "auk_gephi = auk_fp + coll_id + \"-gephi.gexf\"\n",
    "auk_graphml = auk_fp + coll_id + \"-gephi.graphml\"\n",
    "auk_domains = auk_fp + coll_id + \"-fullurls.txt\"\n",
    "auk_filtered_text = auk_fp + coll_id + \"-filtered_text.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Text Analysis\n",
    "\n",
    "The following set of functions use the [Natural Language Toolkit](https://www.nltk.org) Python library to search for the top most used words in the collection, as well as facilitate breaking it down by name or domain.\n",
    "\n",
    "Set the variables below if you wish to make some changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  CONSTANTS / CONFIGURATION\n",
    "#\n",
    "# If you wish to fine tune the outputs, you may change the following:\n",
    "#\n",
    "# maximum number of words to show in output.\n",
    "# Jupyter will create an output error if the number is too high.\n",
    "TOP_COUNT = 30 \n",
    "\n",
    "# Domain suffixes to check non-U.S. domains.\n",
    "# so that (e.g.) www.google.co.uk will become \"google\"\n",
    "STOP_DOMAINS = [\"co\", \"org\", \"net\", \"edu\"] # domain suffixes to remove\n",
    "\n",
    "# minimum number of characters for a word to be included in a corpus\n",
    "MINIMUM_WORD_LENGTH = 3 # eliminates \"it\", \"I\", \"be\" etc.\n",
    "\n",
    "# list of substrings to filter a text line, if desired\n",
    "LINE_FILTER = ['404 Not Found']\n",
    "\n",
    "# The number of the last line of text to ingest\n",
    "RESULTS_LIMIT = 2500\n",
    "\n",
    "# If you want to start ingesting at a different line, you can increase this.\n",
    "# If RESULTS_START is great than RESULTS_LIMIT you will get no results.\n",
    "RESULTS_START = 0\n",
    "\n",
    "# If you have a large file but want to sample the file more broadly, you\n",
    "# can increase this value skip to every Nth line.\n",
    "RESULTS_STEP = 5\n",
    "\n",
    "# change if you want a different filename.\n",
    "OUTPUT_FILENAME = \"./filtered_text.txt\" # filename if you want to output to another file.\n",
    "\n",
    "# characters to show per text file in output. Larger numbers will results in more\n",
    "# text showing in output\n",
    "MAX_CHARACTERS = 75\n",
    "\n",
    "# The years to include in the analysis. If empty, you will get all available years.\n",
    "FILTERED_YEARS = [] # e.g. ['2015', '2016', '2019']\n",
    "\n",
    "# The domains to include in the analysis. If empty, you will get all available domains.\n",
    "FILTERED_DOMAINS = [] # e.g [\"google\", \"apple\", \"facebook\"]\n",
    "\n",
    "# List of words not to include in a corpus for text analysis\n",
    "STOP_WORDS = ['this', 'that', 'with', 'from', 'your']\n",
    "\n",
    "## Toolkit imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ggplot as ggp\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.draw.dispersion import dispersion_plot as dp\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def clean_domain(s):\n",
    "    \"\"\"Extracts the name from the domain (e.g. 'www.google.com' becomes 'google').\"\"\"\n",
    "    ret = \"\"\n",
    "    dom = s.split(\".\")\n",
    "    if len(dom) <3: # x.com is always x\n",
    "        ret = dom[0]\n",
    "    elif dom[-2] in STOP_DOMAINS: # www.x.co.uk should be x\n",
    "        ret = dom[-3]\n",
    "    else:\n",
    "        ret = dom[1]\n",
    "    return ret\n",
    "\n",
    "def get_domains(split_method=\"full\"):\n",
    "    \"\"\"Extracts the domains from a file by method..\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    split_method: Either \"full\" \"name\" or \"sub\". \"name\" provides just the domain name, \n",
    "         \"sub\" produces the name with subdomains. \"full\" provides the entire name.    \n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    with open(auk_domains) as fin:\n",
    "        for line in fin:\n",
    "            ret.append(line.strip('()\\n').split(\",\"))\n",
    "    if split_method == 'name':\n",
    "        scores = Counter()\n",
    "        for url, count in ret:\n",
    "            scores.update({clean_domain(url): int(count)})\n",
    "        ret = scores\n",
    "    elif split_method == 'sub':\n",
    "        scores = Counter()\n",
    "        splits = [(x[0].split('.'), int(x[1])) for x in ret]\n",
    "        for url, count in splits:\n",
    "            if len(url) < 3:\n",
    "                scores.update({'.'.join(['www', url[0]]) : count})\n",
    "            else:\n",
    "                scores.update({ '.'.join([url[0], url[1]]) : count})\n",
    "        ret = scores\n",
    "    else:\n",
    "        scores = Counter()\n",
    "        for url, count in ret:\n",
    "            scores.update({url: int(count)})\n",
    "        ret = scores\n",
    "    return ret\n",
    "\n",
    "def get_text(by=\"all\", minline=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Get just the text from the files.\"\"\"\n",
    "    text = []\n",
    "    form = range(RESULTS_START, RESULTS_LIMIT, RESULTS_STEP)\n",
    "    with open(auk_full_text) as fin:\n",
    "        for num in range(RESULTS_LIMIT):\n",
    "            if num in form:\n",
    "                line = next(fin)\n",
    "                split_line = str(line).split(\",\", 3)\n",
    "                if (len(split_line[3]) >= MINIMUM_WORD_LENGTH and set([split_line[3].find(x) for x in LINE_FILTER]) == {-1}):   \n",
    "                    if by == \"domain\": \n",
    "                        text.append((clean_domain(split_line[1]), split_line[3]))\n",
    "                    elif by == \"year\":\n",
    "                        text.append((split_line[0][1:5], split_line[3]))\n",
    "                    else:\n",
    "                        text.append(split_line[3])\n",
    "            else:\n",
    "                next(fin)                   \n",
    "    return text\n",
    "\n",
    "def get_text_tokens (minlen=MINIMUM_WORD_LENGTH) :\n",
    "    \"\"\"Get the data and tokenize the text.\"\"\"\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        head = [next(fin) for x in range(RESULTS_LIMIT)]\n",
    "    for line in head:\n",
    "        split_line = str(line).lower().split(\",\", 3)\n",
    "        # remove lines that are smaller than 3 chars or in the FILTER list.\n",
    "        if (len(split_line[3]) >= minlen and set([split_line[3].find(x) for x in LINE_FILTER]) == {-1}):\n",
    "            tokens += word_tokenize(split_line[3])\n",
    "    tokens = [x for x in tokens if len(x) > minlen]\n",
    "    return tokens\n",
    "\n",
    "def get_tokens_domains(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Get tokens by domain\"\"\"\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        head = [next(fin) for x in range(RESULTS_LIMIT)]\n",
    "    for line in head:\n",
    "        split_line = str(line).lower().split(\",\", 3)\n",
    "        if (len(split_line[3]) >= minlen and set([split_line[3].find(x) for x in LINE_FILTER]) == {-1}):\n",
    "            tokens.append((clean_domain(split_line[1]), Counter([x for x in word_tokenize(str(split_line[3])) if len(x) > minlen])))\n",
    "    return tokens\n",
    "\n",
    "def get_tokens_years(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Get tokens by year.\"\"\"\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        head = [next(fin) for x in range(RESULTS_LIMIT)]\n",
    "    for line in head:\n",
    "        split_line = str(line).lower().split(\",\", 3)\n",
    "        if (len(split_line[3]) >= minlen and set([split_line[3].find(x) for x in LINE_FILTER]) == {-1}):\n",
    "            tokens.append((split_line[0][1:5], Counter([x for x in word_tokenize(str(split_line[3])) if len(x) > minlen])))\n",
    "    return tokens\n",
    "\n",
    "def year(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Used by get_top_tokens_by to get the tokens by year.\"\"\"\n",
    "    return get_tokens_years(minlen)\n",
    "\n",
    "def domain(minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Used by get_top_tokens_by to get tokens by domain.\"\"\"\n",
    "    return get_tokens_domains(minlen)\n",
    " \n",
    "def get_top_tokens(total=TOP_COUNT, minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\"Return the top tokens for the text.\"\"\"\n",
    "    return [(key, value) for key, value in Counter(get_text_tokens(minlen)).most_common(total)]\n",
    "\n",
    "def get_top_tokens_by(fun, total=TOP_COUNT, minlen=MINIMUM_WORD_LENGTH):\n",
    "    \"\"\" Get the top tokens by a function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fun: A function that returns a list of (key, Counter([tokenized_list]))\n",
    "    total: The number of top tokens to return for each key.\n",
    "    minlen: The minimum word length.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret: list of minlen tokens by fun.\n",
    "    \"\"\"\n",
    "    \n",
    "    sep = dict()\n",
    "    tokens = fun(minlen)\n",
    "    sep = {k[0]: Counter() for k in tokens}\n",
    "    for key, value in tokens:\n",
    "        sep[key] += value\n",
    "    ret = [(key, val.most_common(total)) for key, val in sep.items()]\n",
    "    return (ret)\n",
    "\n",
    "# applies utf-16 encoding to text, if possible\n",
    "def international(text):\n",
    "    unicode = text.encode(\"utf-8\")\n",
    "    try:\n",
    "        ret = unicode.decode(\"UTF-32-LE\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            ret = unicode.decode(\"UTF-32-BE\")\n",
    "        except UnicodeDecodeError:\n",
    "            try: \n",
    "                ret = unicode.decode(\"UTF-16-LE\")\n",
    "            except UnicodeDecodeError:\n",
    "                try: \n",
    "                    ret = unicode.decode(\"UTF-16-BE\")\n",
    "                except UnicodeDecodeError:\n",
    "                    logging.warning(\"Could not convert text to UTF-16 or UTF-32. Restoring UTF-8\")\n",
    "                    ret = unicode.decode(\"UTF-8\")\n",
    "            \n",
    "    return ret\n",
    "\n",
    "# writes results to stdout\n",
    "def write_output (stdout, results):\n",
    "    try:\n",
    "        with open(filename, \"w\") as output:\n",
    "            for value in results:\n",
    "                output_file_write(str(value))\n",
    "    except:\n",
    "        print(\"Error writing the file.\")\n",
    "        \n",
    "def sentiment_scores(by=\"domain\"):\n",
    "    sep = dict()\n",
    "    corpus = get_text(by)\n",
    "    sep = {k[0]: [] for k in corpus}\n",
    "    for key, value in corpus:\n",
    "        sep[key] += sent_tokenize(value)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    result = []\n",
    "    for a, b in sep.items():\n",
    "        scores = Counter({\"neg\": 0, \"pos\":0, \"neu\":0, \"compound\":0})\n",
    "        for c in b:\n",
    "            scores.update(sid.polarity_scores(c))\n",
    "        result += [(a, (\"neg\", scores['neg']/len(b)), (\"pos\", scores['neg']/len(b)), (\"neu\", scores['neu']/len(b)), (\"compound\", scores['compound']/len(b)))]\n",
    "      #  [(key, sid.polarity_scores(text)) for key, text in sep.items()]\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Analysis of Domains\n",
    "\n",
    "Domains data provides basic information about what is crawled and how often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE = ['google', 'facebook', 'youtube', 'apple']\n",
    "plt.rcParams['figure.figsize'] = [10, 4] # set the figure size for the graph\n",
    "\n",
    "# Get a list of the top words in the collection\n",
    "# (regardless of year).\n",
    "\n",
    "domains = get_domains('name').most_common(30) # Can choose 'sub' for subdomains\n",
    "\n",
    "vals = [x[1] for x in domains if x[0] not in EXCLUDE]\n",
    "labs = [x[0] for x in domains if x[0] not in EXCLUDE]\n",
    "\n",
    "ind = np.arange(len(vals))    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, vals, width)\n",
    "\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.title('Top domains by count.')\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have saved the above functions, you can now use them in a variety of ways. \n",
    "\n",
    "### Text by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"year\" # choose \"year\", \"domain\" or \"all\"\n",
    "\n",
    "# Get the set of available years in the collection \n",
    "year_range = set([x[0] for x in get_text(method)])\n",
    "print(year_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create separate lists with text files from individual years in this collection. You can add years (e.g. \"2019\") to `filtered_range` to limit your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_filter = FILTERED_YEARS if FILTERED_YEARS else year_range # add or remove years for filter\n",
    "year_results = [t[1] for t in get_text(\"year\") if t[0] in list(year_filter)]\n",
    "    \n",
    "# Some of the text may be in an international font.\n",
    "for i in year_results[:5]:\n",
    "    print(international(i)[:MAX_CHARACTERS]) # first 50 characters in output\n",
    "\n",
    "## Commenting out the following will write the results to a `output_filename\n",
    "\n",
    "#write_output(output_filename, year_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of available domains in the collection \n",
    "domain_set = set([x[0] for x in get_text(\"domain\")])\n",
    "print(domain_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the given domain to a file and see how many results there are\n",
    "\n",
    "domain_set = FILTERED_DOMAINS if FILTERED_DOMAINS else domain_set\n",
    "domain_results = [t[1] for t in get_text(\"domain\") if t[0] in domain_set]\n",
    "print(\"****Total files****:\" + str(len(domain_results)) + \"\\n\")\n",
    "print(\"****Sample results****: \\n\")\n",
    "for samp in domain_results[:10]:\n",
    "    if len(samp) > 4:\n",
    "        print(international(samp)[:MAX_CHARACTERS] + \" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Collection Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the top words in the collection\n",
    "# (regardless of year).\n",
    "tokens = get_top_tokens()[:20]\n",
    "\n",
    "vals = [x[1] for x in tokens if x[0] not in STOP_WORDS]\n",
    "labs = [x[0] for x in tokens if x[0] not in STOP_WORDS]\n",
    "\n",
    "ind = np.arange(len(vals))    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, vals, width)\n",
    "\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.title('Top words by count.')\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the top tokens, separated by year.\n",
    "get_top_tokens_by(year, 3)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of top tokens, separated by domain.\n",
    "get_top_tokens_by(domain, 3)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dispersion plot, showing where the list of words appear\n",
    "# in the text.\n",
    "text = get_text_tokens(1) # Need to have one to include words with fewer than 3 letters.\n",
    "dp(text, [\"he\", \"she\"]) # uses the nltk dispersion plot library (dp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentiment scores by domain and visualize them.\n",
    "\n",
    "sent = sentiment_scores()\n",
    "N = len(sent)\n",
    "neg = [x[1][1] for x in sent]\n",
    "pos = [x[2][1] for x in sent]\n",
    "neu = [x[3][1] for x in sent]\n",
    "labs = [x[0] for x in sent]\n",
    "\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, neg, width)\n",
    "p2 = plt.bar(ind, neu, width,\n",
    "             bottom=neg)\n",
    "p3 = plt.bar(ind, pos, width, bottom=neu)\n",
    "\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores by domain and sentiment')\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "plt.legend((p1[0], p2[0], p3[0]), ('Negative', 'Neutral', 'Positive'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentiment scores by year and visualize them.\n",
    "\n",
    "sent = sentiment_scores(\"year\")\n",
    "N = len(sent)\n",
    "neg = [x[1][1] for x in sent]\n",
    "pos = [x[2][1] for x in sent]\n",
    "neu = [x[3][1] for x in sent]\n",
    "labs = sorted([x[0] for x in sent])\n",
    "\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, neg, width)\n",
    "p2 = plt.bar(ind, neu, width,\n",
    "             bottom=neg)\n",
    "p3 = plt.bar(ind, pos, width, bottom=neu)\n",
    "\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores by domain and sentiment')\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "plt.legend((p1[0], p2[0], p3[0]), ('Negative', 'Neutral', 'Positive'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis\n",
    "\n",
    "Sometimes it is useful to include information about the relationships among different websites. We recommend using [Gephi](https://gephi.org/) for graph visualization, but it is possible in this notebook using the Python networkx library as well.\n",
    "\n",
    "You may need to adjust the node size, label size and figure size parameters for the layout to look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4] # set the figure size for the graph\n",
    "\n",
    "NETWORK_EXCLUDE = [\"google.com\"]\n",
    "graph = nx.read_gexf(auk_gephi) #import the graph\n",
    "\n",
    "# Degree distribution for the graph\n",
    "\n",
    "g_nodes = zip([x[1] for x in graph.nodes('label')], [x[1] for x in graph.nodes('Degree')])\n",
    "\n",
    "gs = sorted([x for x in g_nodes], key=lambda s: s[1], reverse=True)\n",
    "\n",
    "vals = [x[1] for x in gs if x[0] not in NETWORK_EXCLUDE][:10]\n",
    "labs = [x[0] for x in gs if x[0] not in NETWORK_EXCLUDE][:10]\n",
    "\n",
    "ind = np.arange(len(vals))    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, vals, width)\n",
    "\n",
    "plt.ylabel('Degree')\n",
    "plt.title('Top domains by Degree.')\n",
    "plt.xticks(ind, labs, rotation='vertical')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [25, 25] # set the figure size for the graph\n",
    "\n",
    "# Convert rgb values to between 0 & 1 and put them into a numpy array.\n",
    "rgbs = zip([x[1]/255 for x in graph.nodes('r')], [x[1]/255 for x in graph.nodes('g')], [x[1]/255 for x in graph.nodes('b')])\n",
    "colormap = [np.array(x) for x in rgbs]\n",
    "\n",
    "# Labels\n",
    "mapping = {x[0]: x[1] for x in graph.nodes('label')}\n",
    "\n",
    "# Use Archive Unleashed Clouds Positions (saves on load time)\n",
    "zippos = zip(graph.nodes, [x[1] for x in graph.nodes('x')], [x[1] for x in graph.nodes('y')])\n",
    "positions = {x[0]: np.array([x[1],x[2]]) for x in zippos}\n",
    "\n",
    "# Node sizes based on degree\n",
    "size = np.array([x[1] * 100 for x in graph.nodes('size')])\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(graph, pos=positions, show_labels=True, labels=mapping, font_size=10, node_size=size, node_color=colormap)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ego network for a particular node\n",
    "\n",
    "largest_node = sorted(graph.nodes('Degree'), key=lambda s: s[1], reverse=True)[0][0] # [1][0] is second largest, etc\n",
    "neigh = graph.subgraph(graph.neighbors(largest_node))\n",
    "# Convert rgb values to between 0 & 1 and put them into a numpy array.\n",
    "rgbs = zip([x[1]/255 for x in neigh.nodes('r')], [x[1]/255 for x in neigh.nodes('g')], [x[1]/255 for x in neigh.nodes('b')])\n",
    "colormap = [np.array(x) for x in rgbs]\n",
    "\n",
    "# Labels\n",
    "mapping = {x[0]: x[1] for x in neigh.nodes('label')}\n",
    "\n",
    "# Use Archive Unleashed Clouds Positions (saves on load time)\n",
    "zippos = zip(neigh.nodes, [x[1] for x in neigh.nodes('x')], [x[1] for x in neigh.nodes('y')])\n",
    "positions = {x[0]: np.array([x[1],x[2]]) for x in zippos}\n",
    "\n",
    "# Node sizes based on degree\n",
    "size = np.array([x[1] * 100 for x in neigh.nodes('size')])\n",
    "\n",
    "nx.draw(neigh, pos=positions, show_labels=True, labels=mapping, font_size=10, node_size=size, node_color=colormap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "Bird, Steven, Edward Loper and Ewan Klein (2009), *Natural Language       Processing with Python*. O’Reilly Media Inc.\n",
    "\n",
    "Archives Unleashed Project. (2018). Archives Unleashed Toolkit (Version 0.17.0). Apache License, Version 2.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
