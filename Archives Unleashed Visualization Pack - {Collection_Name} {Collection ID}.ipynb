{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://cloud.archivesunleashed.org/assets/logo-8d2126e162dc682078284bb8f5585e4365fbad6dc04aa2afbae747626bd815ea.png\" height=\"100px\" width=\"100px\">\n",
    "\n",
    "# Welcome\n",
    "\n",
    "Welcome to the Archives Unleashed Cloud Visualization Demo in Jupyter Notebook for your collection { Collection Name } id { Collection Id }. \n",
    "\n",
    "This demonstration takes the main derivatives from the Archives Unleashed Cloud and uses some common Python data analysis approaches to produce some basic information about the collection you provided.\n",
    "\n",
    "This product is in beta, so if you encounter any issues, please post an [issue in our Github repository](https://github.com/archivesunleashed/auk/issues) to let us know what went wrong.\n",
    "\n",
    "If you have some basic Python coding experience, you can change the code we provided to suit your own needs.\n",
    "\n",
    "### How Jupyter Notebooks Work:\n",
    "\n",
    "If you have no previous experience of Jupyter Notebooks, the most important thing to understand is that that <Shift><Enter/Return> will run the python code inside a window and output it to the site.\n",
    "    \n",
    "The window titled `\"#RUN THIS FIRST\"` should be the first place you go. This will import all the libraries and set basic variables (eg. where your derivative files are located) for the notebook. After that, everything else should be able to run on its own.\n",
    "\n",
    "Unfortunately, we cannot support code that you produced yourself. We recommend that you use `File > Make a Copy` first before changing the code in the repository. That way, you can always return to the basic visualizations we have offered here. Of course, you can also just re-download the Jupyter Notebook service from your Archives Unleashed Cloud account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS FIRST\n",
    "# This Window will set up all the necessary libraries and dependencies\n",
    "# for your Collection.\n",
    "coll_id = \"4656\"\n",
    "auk_fp = \"data/\"\n",
    "auk_full_text = auk_fp + coll_id + \"-fulltext.txt\"\n",
    "auk_gephi = auk_fp + \"coll_id-gephi.gexf\"\n",
    "auk_graphml = auk_fp + \"coll_id-gephi.grapml\"\n",
    "auk_domains = auk_fp + \"coll_id-fullurls.txt\"\n",
    "auk_filtered_text = auk_fp + \"coll_id-filtered_text.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script will attempt to install the necessary dependencies\n",
    "# for the visualisations. You may prefer to install these on your\n",
    "# own in the command line.\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.draw.dispersion import dispersion_plot as dp\n",
    "import wordcloud as wc\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "The following function uses the nltk python library to search for the top most used words in the collection. Depending on the size of the text file, it may take a while to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the value of top to get more results.\n",
    "# Citation for nltk library: \n",
    "top = 30\n",
    "\n",
    "def get_textfile () :\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            tokens += word_tokenize(str(line).split(\",\")[3])\n",
    "    return tokens\n",
    "\n",
    "def get_text_years():\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            split_line = str(line).split(',')\n",
    "            tokens.append((split_line[0][1:5], split_line[3]))\n",
    "    return tokens\n",
    "\n",
    "def get_top_tokens(total=20):\n",
    "    tokens = get_textfile()\n",
    "    tokens = [(value, key) for key, value in Counter(tokens).items()]\n",
    "    tokens = list(filter(lambda x : len(x[1]) > 3, tokens))\n",
    "    tokens.sort(reverse=True)\n",
    "    return(tokens[0:total])\n",
    "\n",
    "def get_top_tokens_by_year(total=20):\n",
    "    tokens = get_text_years()\n",
    "    sep = {key: \"\" for key, value in tokens}\n",
    "    for year, text in tokens:\n",
    "            sep[str(year)] = sep[str(year)] + \" \" + text\n",
    "    ret = [(key, Counter(word_tokenize(val)).most_common(total)) for key, val in sep.items()]\n",
    "    return (ret)\n",
    "\n",
    "# Get a list of the top words in the collection.\n",
    "get_top_tokens(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the top tokens, separated by year.\n",
    "get_top_tokens_by_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_textfile()\n",
    "dp(text, [\"he\", \"she\"]) # dispersion plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "Bird, Steven, Edward Loper and Ewan Klein (2009), *Natural Language       Processing with Python*. Oâ€™Reilly Media Inc.\n",
    "\n",
    "Archives Unleashed Project. (2018). Archives Unleashed Toolkit (Version 0.17.0). Apache License, Version 2.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
